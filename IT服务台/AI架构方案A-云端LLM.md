# 方案A：AI中台架构（云端LLM + RAG）

## 一、架构设计

### 整体架构图

```
┌─────────────────────────────────────────────────────────────┐
│                   用户交互层                                   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  对话式UI（ChatGPT风格）                               │   │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐          │   │
│  │  │User门户   │  │工程师台  │  │管理后台  │          │   │
│  │  │(聊天+表单)│  │(AI助手)  │  │(报表)    │          │   │
│  │  └──────────┘  └──────────┘  └──────────┘          │   │
│  │  - WebSocket实时通信                                  │   │
│  │  - Markdown渲染                                       │   │
│  │  - 语音输入（可选）                                    │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                         ↕ WebSocket/SSE
┌─────────────────────────────────────────────────────────────┐
│                  API网关 + BFF层                              │
│  - 路由、鉴权、限流                                           │
│  - 对话历史聚合                                               │
│  - AI响应流式转发                                             │
└─────────────────────────────────────────────────────────────┘
                         ↕
┌─────────────────────────────────────────────────────────────┐
│               AI中台层（核心智能引擎）                          │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  对话编排服务（Orchestration Service）                 │   │
│  │  - 意图识别路由                                        │   │
│  │  - 多轮对话管理                                        │   │
│  │  - 上下文维护（Redis）                                 │   │
│  └──────────────────────────────────────────────────────┘   │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐   │
│  │智能问答   │  │工单助手  │  │知识助手  │  │诊断助手  │   │
│  │Agent     │  │Agent     │  │Agent     │  │Agent     │   │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘   │
│        ↓              ↓              ↓              ↓        │
│  ┌──────────────────────────────────────────────────────┐   │
│  │           RAG引擎（检索增强生成）                      │   │
│  │  ┌────────────┐  ┌────────────┐  ┌────────────┐    │   │
│  │  │Query重写   │  │混合检索    │  │重排序      │    │   │
│  │  └────────────┘  └────────────┘  └────────────┘    │   │
│  └──────────────────────────────────────────────────────┘   │
│                         ↕                                    │
│  ┌──────────────────────────────────────────────────────┐   │
│  │           LLM服务层（统一接口）                        │   │
│  │  ┌────────────┐  ┌────────────┐  ┌────────────┐    │   │
│  │  │OpenAI API  │  │通义千问API │  │文心一言API │    │   │
│  │  │(GPT-4)     │  │(Qwen)      │  │(ERNIE)     │    │   │
│  │  └────────────┘  └────────────┘  └────────────┘    │   │
│  │  - Prompt模板管理                                     │   │
│  │  - Token计数与成本控制                                │   │
│  │  - 请求缓存（相似问题）                               │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                         ↕
┌─────────────────────────────────────────────────────────────┐
│                  业务服务层                                   │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐   │
│  │工单服务   │  │用户服务  │  │通知服务  │  │集成服务  │   │
│  │Ticket    │  │User      │  │Notify    │  │Integration│  │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘   │
└─────────────────────────────────────────────────────────────┘
                         ↕
┌─────────────────────────────────────────────────────────────┐
│                  数据持久层                                   │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐   │
│  │MySQL     │  │Redis     │  │Milvus    │  │Elasticsearch│  │
│  │业务数据   │  │会话+缓存 │  │向量数据库│  │知识库全文  │  │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘   │
│  ┌──────────┐  ┌──────────┐                                │
│  │MongoDB   │  │MinIO/OSS │                                │
│  │日志+对话 │  │附件存储  │                                │
│  └──────────┘  └──────────┘                                │
└─────────────────────────────────────────────────────────────┘
```

## 二、技术栈详解

### 2.1 前端层

```yaml
框架: Vue 3 + TypeScript
UI组件:
  - 对话组件: 自研ChatUI（参考ChatGPT风格）
  - 表单组件: Element Plus（传统表单作为补充）
实时通信: WebSocket（Socket.io）
语音: Web Speech API / 讯飞语音SDK
状态管理: Pinia
Markdown: markdown-it
代码高亮: highlight.js
```

### 2.2 AI中台层

```yaml
编排框架: LangChain / Semantic Kernel
开发语言: Python 3.11 + FastAPI

Agent实现:
  - 框架: LangChain Agents
  - 工具: Custom Tools（工单CRUD、知识检索等）

RAG引擎:
  - 框架: LlamaIndex / LangChain
  - Embedding: OpenAI text-embedding-ada-002 / BGE-M3
  - 向量数据库: Milvus 2.3 / Qdrant
  - 重排序: Cohere Rerank / BGE-Reranker

LLM接入:
  - 主力: OpenAI GPT-4 Turbo（API）
  - 备用: 阿里云通义千问、百度文心一言
  - SDK: LangChain统一接口

Prompt管理:
  - Prompt模板: Jinja2
  - 版本控制: Git + PromptLayer
  - A/B测试: 自研平台
```

### 2.3 业务服务层

```yaml
工单服务: Go + Gin（高性能）
用户服务: Java + Spring Boot（企业集成）
通知服务: Node.js + Express（实时推送）
集成服务: Python + FastAPI（灵活对接）
```

### 2.4 数据层

```yaml
关系数据库: MySQL 8.0（主从）
缓存: Redis 7.x Cluster
  - 对话上下文（TTL 30分钟）
  - LLM响应缓存（相似度>0.95）
向量数据库: Milvus 2.3
  - 知识库向量（维度1536）
  - 历史工单向量（用于智能推荐）
搜索引擎: Elasticsearch 8.x
  - 知识库全文检索
  - 工单日志分析
文档数据库: MongoDB
  - 对话历史完整记录
  - AI决策过程日志
对象存储: MinIO / OSS
  - 附件、截图
  - 语音文件
```

## 三、AI能力实现细节

### 3.1 智能问答流程

```python
# 伪代码示例
def intelligent_qa(user_query: str, session_id: str):
    # 1. 检索相关知识（RAG）
    contexts = rag_engine.retrieve(
        query=user_query,
        top_k=5,
        filters={"type": "faq"}
    )

    # 2. 构建Prompt
    prompt = f"""
    你是IT服务台的智能助手。

    用户问题: {user_query}

    相关知识:
    {contexts}

    请基于以上知识回答用户问题。如果知识库中没有答案，引导用户创建工单。
    """

    # 3. 调用LLM
    response = llm.chat(
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        stream=True
    )

    # 4. 流式返回
    for chunk in response:
        yield chunk

    # 5. 记录对话
    save_conversation(session_id, user_query, response)
```

### 3.2 工单智能生成

```python
def auto_generate_ticket(conversation_history: List):
    # 从对话历史提取工单信息
    extraction_prompt = """
    从以下对话中提取工单信息:

    对话历史: {conversation_history}

    请以JSON格式输出:
    {
        "title": "工单标题",
        "description": "详细描述",
        "category": "问题类型",
        "priority": "紧急程度(高/中/低)",
        "attachments": ["相关截图URL"]
    }
    """

    ticket_info = llm.chat(
        prompt=extraction_prompt,
        response_format={"type": "json"}
    )

    return ticket_info
```

### 3.3 智能派单

```python
def intelligent_dispatch(ticket_info: dict):
    # 基于工单内容和工程师技能匹配
    engineers = get_available_engineers()

    # 向量检索最匹配的工程师
    ticket_embedding = embedding_model.embed(
        ticket_info["description"]
    )

    # 在工程师技能向量中检索
    matched_engineers = vector_db.search(
        collection="engineer_skills",
        vector=ticket_embedding,
        top_k=3
    )

    # 结合负载均衡
    best_engineer = select_by_workload(matched_engineers)

    return best_engineer
```

## 四、核心优势

| 优势维度 | 具体说明 |
|---------|---------|
| **对话式体验** | 类ChatGPT的自然交互，用户无需学习使用 |
| **智能化程度高** | RAG+LLM实现真正理解用户意图 |
| **快速上线** | 云端LLM无需训练，2-3个月可上线 |
| **成本可控** | 缓存+相似度匹配减少API调用50% |
| **易维护** | Prompt工程，无需模型训练和GPU运维 |
| **扩展性强** | 云端算力无限扩展 |
| **效果最佳** | GPT-4能力业界领先 |

## 五、成本分析

### 5.1 月度成本（500用户，20万次请求）

```
OpenAI GPT-4 Turbo:
  - 输入: $10/1M tokens
  - 输出: $30/1M tokens
  - 平均每次请求: 500输入 + 300输出 tokens
  - 成本: (500×10 + 300×30) / 1M × 200K = $2,400 ≈ ¥17,000

缓存优化（命中率40%）:
  - 实际成本: ¥17,000 × 60% = ¥10,200

基础设施:
  - 服务器+Redis+Milvus: ¥5,000

总成本: ¥15,200/月
```

### 5.2 不同规模成本

| 月请求量 | 云端LLM成本 | 基础设施 | 总成本 |
|---------|------------|---------|--------|
| 10万次 | ¥5,100 | ¥5,000 | ¥10,100 |
| 20万次 | ¥10,200 | ¥5,000 | ¥15,200 |
| 50万次 | ¥25,500 | ¥8,000 | ¥33,500 |
| 100万次 | ¥51,000 | ¥10,000 | ¥61,000 |

**成本优化策略：**
- 缓存优化：相似度>0.95直接返回缓存，节省40-50%
- 分流策略：简单FAQ用规则引擎，节省30%
- 模型选择：一般问题用GPT-3.5-Turbo（成本1/10）
- 批量处理：非实时任务批量调用

## 六、技术风险与应对

### 6.1 核心风险

| 风险 | 可能性 | 影响 | 应对措施 |
|------|-------|------|---------|
| **依赖云端API** | 高 | 高 | ①多云备用（通义千问、文心一言）②本地降级方案 |
| **网络延迟** | 中 | 中 | ①CDN加速 ②边缘节点部署 ③缓存优化 |
| **成本不可控** | 中 | 高 | ①设置每日预算上限 ②实时成本监控 ③限流保护 |
| **数据安全** | 中 | 高 | ①敏感信息脱敏 ②数据加密传输 ③审计日志 |
| **Prompt注入攻击** | 中 | 高 | ①输入过滤 ②输出检测 ③安全审计 |
| **服务稳定性** | 低 | 高 | ①多云容灾 ②降级方案（规则引擎） |

### 6.2 数据安全方案

```python
# 敏感信息脱敏
def desensitize(text: str):
    # 手机号脱敏
    text = re.sub(r'1[3-9]\d{9}', '***********', text)
    # 邮箱脱敏
    text = re.sub(r'\w+@\w+\.\w+', '***@***.***', text)
    # 身份证脱敏
    text = re.sub(r'\d{17}[\dXx]', '******************', text)
    return text

# API调用前处理
def safe_llm_call(user_input: str):
    # 1. 敏感信息检测
    if contains_sensitive(user_input):
        user_input = desensitize(user_input)

    # 2. Prompt注入检测
    if is_prompt_injection(user_input):
        return "检测到异常输入，请重新输入"

    # 3. 调用LLM
    response = llm.chat(user_input)

    # 4. 输出检测
    if contains_harmful_content(response):
        return "抱歉，无法回答该问题"

    return response
```

## 七、适合场景

### 7.1 最适合

✅ **快速MVP验证**：2-3个月快速上线
✅ **团队AI能力弱**：无需GPU运维和模型调优
✅ **预算充足**：月度成本1.5-3万可接受
✅ **效果要求高**：需要GPT-4级别的智能
✅ **100-2000用户**：中小规模场景
✅ **云优先战略**：全面拥抱云服务

### 7.2 不适合

❌ **纯离线环境**：无法连接互联网
❌ **数据安全极高**：不能将数据发送到云端
❌ **成本极敏感**：无法承受按量付费
❌ **超大规模**：>5000用户，成本激增
❌ **政府/金融**：严格的数据合规要求

## 八、实施路线

### 阶段1：基础搭建（0-1月）

```
前端开发:
  - 对话UI组件
  - WebSocket集成
  - Markdown渲染
  工期: 2周

后端开发:
  - FastAPI框架搭建
  - OpenAI API集成
  - 基础RAG实现
  工期: 3周

知识库准备:
  - FAQ数据准备（100条）
  - 向量化存储
  工期: 1周

测试部署:
  - 功能测试
  - 性能测试
  工期: 1周
```

### 阶段2：功能完善（1-2月）

```
AI能力增强:
  - 工单智能生成
  - 智能派单
  - 知识推荐

业务集成:
  - 与现有系统对接
  - 统一认证
  - 数据同步

优化迭代:
  - Prompt优化
  - 缓存策略
  - 成本优化
```

### 阶段3：上线运营（2-3月）

```
灰度发布:
  - 10% -> 30% -> 50% -> 100%

监控告警:
  - 成本监控
  - 性能监控
  - 质量监控

持续优化:
  - badcase收集
  - Prompt迭代
  - 知识库扩充
```

## 九、总结

### 核心定位

方案A定位为**快速验证型方案**，适合：
- 需要快速上线验证AI能力
- 团队AI技术积累较少
- 预算允许按量付费模式
- 对效果要求高（GPT-4能力）

### 关键指标

- **上线时间**：2-3个月
- **初期投入**：¥5-10万（无需GPU）
- **月度成本**：¥15-32K（500用户）
- **AI能力**：⭐⭐⭐⭐⭐（GPT-4最强）
- **响应速度**：1-3秒
- **用户自助率**：60-70%

### 演进路径

如果后期需要降低成本或提升私有化能力，可以：
1. 引入本地LLM作为Level 2（参考方案D）
2. 扩大缓存和规则引擎覆盖率
3. 将云端LLM作为复杂问题兜底
4. 逐步过渡到混合架构
